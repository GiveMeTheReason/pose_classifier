{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import model.classifiers as classifiers\n",
    "import model.transforms as transforms\n",
    "\n",
    "import utils.utils_mediapipe as utils_mediapipe\n",
    "from config import DATA_CONFIG, TRAIN_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 1\n",
    "device = 'cpu'\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    TRAIN_CONFIG.train_params.output_data,\n",
    "    f'experiment_{str(exp_id).zfill(3)}',\n",
    "    'checkpoint.pth',\n",
    ")\n",
    "\n",
    "samples_folder = DATA_CONFIG.mediapipe.points_unified_world_filtered_labeled\n",
    "\n",
    "label_map = TRAIN_CONFIG.gesture_set.label_map\n",
    "inv_label_map = TRAIN_CONFIG.gesture_set.inv_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (positional_embeddings): PositionalEncoding()\n",
       "  (linear1): Linear(in_features=30, out_features=256, bias=True)\n",
       "  (lstm1): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "  (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = TRAIN_CONFIG.transforms_params.to_keep\n",
    "shape_limit = TRAIN_CONFIG.transforms_params.shape_limit\n",
    "\n",
    "test_transforms = transforms.TestTransforms(\n",
    "    to_keep=to_keep,\n",
    "    shape_limit=shape_limit,\n",
    "    device=device,\n",
    ")\n",
    "label_transforms = transforms.LabelsTransforms(\n",
    "    shape_limit=shape_limit,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = classifiers.LSTMClassifier(sum(to_keep), len(label_map))\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 101\n",
    "gesture = 'select'\n",
    "hand = 'left'\n",
    "trial = 1\n",
    "\n",
    "file_path = os.path.join(\n",
    "    samples_folder,\n",
    "    f'G{subject}_{gesture}_{hand}_trial{trial}.npy'\n",
    ")\n",
    "\n",
    "data = utils_mediapipe.load_points(file_path)\n",
    "\n",
    "points = test_transforms(data[:, :-1])\n",
    "labels = label_transforms(data[:, -1] * label_map[gesture])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction = model(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_probs, prediction_labels = prediction.max(dim=-1)\n",
    "prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100.00%'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (prediction_labels == labels).sum() / len(labels)\n",
    "f'{accuracy.item():.2%}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import typing as tp\n",
    "\n",
    "import cv2\n",
    "import pyk4a\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import model.classifiers as classifiers\n",
    "import model.transforms as transforms\n",
    "\n",
    "import utils.utils_camera_systems as utils_camera_systems\n",
    "import utils.utils_kalman_filter as utils_kalman_filter\n",
    "import utils.utils_mediapipe as utils_mediapipe\n",
    "import utils.utils_unified_format as utils_unified_format\n",
    "from config import DATA_CONFIG, TRAIN_CONFIG, KALMAN_FILTER_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 1\n",
    "device = 'cpu'\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    TRAIN_CONFIG.train_params.output_data,\n",
    "    f'experiment_{str(exp_id).zfill(3)}',\n",
    "    'checkpoint.pth',\n",
    ")\n",
    "\n",
    "samples_folder = DATA_CONFIG.streaming.stream_1\n",
    "\n",
    "label_map = TRAIN_CONFIG.gesture_set.label_map\n",
    "inv_label_map = TRAIN_CONFIG.gesture_set.inv_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "mp_solver_settings = dict(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    ")\n",
    "mp_solver = mp_holistic.Holistic(**mp_solver_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "\n",
    "KALMAN_PARAMS = KALMAN_FILTER_CONFIG.init_params.as_dict()\n",
    "KALMAN_HEURISTICS_FUNC = KALMAN_FILTER_CONFIG.heuristics.as_dict()\n",
    "\n",
    "CAMERA_PARAMS_PATH = os.path.join(\n",
    "    samples_folder,\n",
    "    'calibration_fake.json',\n",
    ")\n",
    "\n",
    "image_size, intrinsic = utils_camera_systems.get_camera_params(CAMERA_PARAMS_PATH)\n",
    "camera_systems = utils_camera_systems.CameraSystems(image_size, intrinsic)\n",
    "depth_extractor = utils_camera_systems.DepthExtractor(WINDOW_SIZE)\n",
    "\n",
    "kfs = []\n",
    "for i in range(utils_unified_format.TOTAL_POINTS_COUNT):\n",
    "    point = i\n",
    "    if point >= 18:\n",
    "        point = 4\n",
    "    params = KALMAN_FILTER_CONFIG.init_params.as_dict()\n",
    "    params['sigma_u'] = params.pop('sigma_u_points')[point]\n",
    "    params['init_Q'] = np.copy(params['init_Q']) * (params['sigma_u'] ** 2)\n",
    "    kfs.append(utils_kalman_filter.KalmanFilter(**params, **KALMAN_HEURISTICS_FUNC))\n",
    "kalman_filters = utils_kalman_filter.KalmanFilters(kfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (positional_embeddings): PositionalEncoding()\n",
       "  (linear1): Linear(in_features=30, out_features=256, bias=True)\n",
       "  (lstm1): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "  (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = TRAIN_CONFIG.transforms_params.to_keep\n",
    "\n",
    "test_transforms = transforms.TestStreamTransforms(\n",
    "    to_keep=to_keep,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = classifiers.LSTMClassifier(sum(to_keep), len(label_map))\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_stream_file = os.path.join(\n",
    "    samples_folder,\n",
    "    'color.mkv',\n",
    ")\n",
    "depth_stream_file = os.path.join(\n",
    "    samples_folder,\n",
    "    'depth.mkv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_cap = cv2.VideoCapture(color_stream_file)\n",
    "depth_cap = cv2.VideoCapture(depth_stream_file)\n",
    "\n",
    "mp_solver.reset()\n",
    "predicted = None\n",
    "\n",
    "while color_cap.isOpened() and depth_cap.isOpened():\n",
    "    color_ret, color_image = color_cap.read()\n",
    "    depth_ret, depth_image = depth_cap.read()\n",
    "    if color_ret and depth_ret:\n",
    "        depth_image = depth_image.T\n",
    "        ### MediaPipe Extractor\n",
    "        ### ------------------------------\n",
    "        landmarks = mp_solver.process(color_image)\n",
    "        joined_landmarks = itertools.chain(\n",
    "            landmarks.pose_landmarks.landmark if landmarks.pose_landmarks is not None else utils_mediapipe.EMPTY_POSE,\n",
    "            landmarks.left_hand_landmarks.landmark if landmarks.left_hand_landmarks is not None else utils_mediapipe.EMPTY_HAND,\n",
    "            landmarks.right_hand_landmarks.landmark if landmarks.right_hand_landmarks is not None else utils_mediapipe.EMPTY_HAND,\n",
    "        )\n",
    "        frame_points = utils_mediapipe.landmarks_to_array(joined_landmarks)[:, :3]\n",
    "        mp_points = frame_points.reshape(-1)\n",
    "\n",
    "        ### Filtration\n",
    "        ### ------------------------------\n",
    "        mp_points = utils_mediapipe.mediapipe_to_unified(\n",
    "            mp_points.reshape(-1, utils_mediapipe.TOTAL_POINTS_COUNT, 3)\n",
    "        ).reshape(-1, 3 * utils_unified_format.TOTAL_POINTS_COUNT)\n",
    "\n",
    "        frame_points = mp_points.reshape(-1, 3)\n",
    "        frame_points = camera_systems.zero_points_outside_screen(\n",
    "            frame_points,\n",
    "            is_normalized=True,\n",
    "            inplace=True,\n",
    "        )\n",
    "        frame_points = camera_systems.normalized_to_screen(\n",
    "            frame_points,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        depths = depth_extractor.get_depth_in_window(\n",
    "            depth_image,\n",
    "            frame_points,\n",
    "            predicted,\n",
    "        )\n",
    "\n",
    "        if predicted is None:\n",
    "            kalman_filters.reset([\n",
    "                np.array([[point], [0]])\n",
    "                for point in depths\n",
    "            ])\n",
    "        depths_filtered = kalman_filters.update(\n",
    "            depths,\n",
    "            use_heuristic=True,\n",
    "            projection=0,\n",
    "        )\n",
    "\n",
    "        predicted = kalman_filters.predict(projection=0)\n",
    "        depths_filtered = tp.cast(tp.List[float], depths_filtered)\n",
    "        predicted = tp.cast(tp.List[float], predicted)\n",
    "\n",
    "        frame_points[:, 2] = depths_filtered\n",
    "        frame_points = camera_systems.screen_to_world(\n",
    "            frame_points,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # cv2.imshow('Color Frame', color_frame)\n",
    "        # cv2.imshow('Depth Frame', depth_frame)\n",
    "\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "color_cap.release()\n",
    "depth_cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from pyk4a import ImageFormat\n",
    "\n",
    "\n",
    "def convert_to_bgra_if_required(color_format: ImageFormat, color_image):\n",
    "    # examples for all possible pyk4a.ColorFormats\n",
    "    if color_format == ImageFormat.COLOR_MJPG:\n",
    "        color_image = cv2.imdecode(color_image, cv2.IMREAD_COLOR)\n",
    "    elif color_format == ImageFormat.COLOR_NV12:\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_YUV2BGRA_NV12)\n",
    "        # this also works and it explains how the COLOR_NV12 color color_format is stored in memory\n",
    "        # h, w = color_image.shape[0:2]\n",
    "        # h = h // 3 * 2\n",
    "        # luminance = color_image[:h]\n",
    "        # chroma = color_image[h:, :w//2]\n",
    "        # color_image = cv2.cvtColorTwoPlane(luminance, chroma, cv2.COLOR_YUV2BGRA_NV12)\n",
    "    elif color_format == ImageFormat.COLOR_YUY2:\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_YUV2BGRA_YUY2)\n",
    "    return color_image\n",
    "\n",
    "\n",
    "def colorize(\n",
    "    image: np.ndarray,\n",
    "    clipping_range: Tuple[Optional[int], Optional[int]] = (None, None),\n",
    "    colormap: int = cv2.COLORMAP_HSV,\n",
    ") -> np.ndarray:\n",
    "    if clipping_range[0] or clipping_range[1]:\n",
    "        img = image.clip(clipping_range[0], clipping_range[1])  # type: ignore\n",
    "    else:\n",
    "        img = image.copy()\n",
    "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    img = cv2.applyColorMap(img, colormap)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\n",
    "    samples_folder,\n",
    "    'output.mkv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "playback = pyk4a.PyK4APlayback(filename)\n",
    "\n",
    "playback.open()\n",
    "calib = playback.calibration_raw\n",
    "calib_json = json.dumps(calib)\n",
    "playback.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/player001/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "playback.open()\n",
    "while True:\n",
    "    try:\n",
    "        frame = playback.get_next_capture()\n",
    "    except EOFError as err:\n",
    "        break\n",
    "\n",
    "    if frame.color is not None:\n",
    "        cv2.imshow(\"Color\", convert_to_bgra_if_required(playback.configuration[\"color_format\"], frame.color))\n",
    "    if frame.depth is not None:\n",
    "        cv2.imshow(\"Depth\", colorize(frame.depth, (None, 5000)))\n",
    "    key = cv2.waitKey(10)\n",
    "    if key != -1:\n",
    "        break\n",
    "\n",
    "playback.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
