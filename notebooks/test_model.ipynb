{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import model.classifiers as classifiers\n",
    "import model.transforms as transforms\n",
    "\n",
    "import utils.utils_mediapipe as utils_mediapipe\n",
    "from config import DATA_CONFIG, TRAIN_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 1\n",
    "device = 'cpu'\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    TRAIN_CONFIG.train_params.output_data,\n",
    "    f'experiment_{str(exp_id).zfill(3)}',\n",
    "    'checkpoint.pth',\n",
    ")\n",
    "\n",
    "samples_folder = DATA_CONFIG.mediapipe.points_unified_world_filtered_labeled\n",
    "\n",
    "label_map = TRAIN_CONFIG.gesture_set.label_map\n",
    "inv_label_map = TRAIN_CONFIG.gesture_set.inv_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (positional_embeddings): PositionalEncoding()\n",
       "  (linear1): Linear(in_features=30, out_features=256, bias=True)\n",
       "  (lstm1): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "  (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = TRAIN_CONFIG.transforms_params.to_keep\n",
    "shape_limit = TRAIN_CONFIG.transforms_params.shape_limit\n",
    "\n",
    "test_transforms = transforms.TestTransforms(\n",
    "    to_keep=to_keep,\n",
    "    shape_limit=shape_limit,\n",
    "    device=device,\n",
    ")\n",
    "label_transforms = transforms.LabelsTransforms(\n",
    "    shape_limit=shape_limit,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = classifiers.LSTMClassifier(sum(to_keep), len(label_map))\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 101\n",
    "gesture = 'select'\n",
    "hand = 'left'\n",
    "trial = 1\n",
    "\n",
    "file_path = os.path.join(\n",
    "    samples_folder,\n",
    "    f'G{subject}_{gesture}_{hand}_trial{trial}.npy'\n",
    ")\n",
    "\n",
    "data = utils_mediapipe.load_points(file_path)\n",
    "\n",
    "points = test_transforms(data[:, :-1])\n",
    "labels = label_transforms(data[:, -1] * label_map[gesture])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction = model(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_probs, prediction_labels = prediction.max(dim=-1)\n",
    "prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100.00%'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (prediction_labels == labels).sum() / len(labels)\n",
    "f'{accuracy.item():.2%}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import pyk4a\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import model.classifiers as classifiers\n",
    "import model.transforms as transforms\n",
    "\n",
    "import utils.utils_camera_systems as utils_camera_systems\n",
    "import utils.utils_kalman_filter as utils_kalman_filter\n",
    "import utils.utils_mediapipe as utils_mediapipe\n",
    "import utils.utils_unified_format as utils_unified_format\n",
    "from config import DATA_CONFIG, TRAIN_CONFIG, KALMAN_FILTER_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = 1\n",
    "device = 'cpu'\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    TRAIN_CONFIG.train_params.output_data,\n",
    "    f'experiment_{str(exp_id).zfill(3)}',\n",
    "    'checkpoint.pth',\n",
    ")\n",
    "\n",
    "samples_folder = DATA_CONFIG.streaming.stream_1\n",
    "\n",
    "label_map = TRAIN_CONFIG.gesture_set.label_map\n",
    "inv_label_map = TRAIN_CONFIG.gesture_set.inv_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "mp_solver_settings = dict(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    ")\n",
    "mp_solver = mp_holistic.Holistic(**mp_solver_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "\n",
    "KALMAN_PARAMS = KALMAN_FILTER_CONFIG.init_params.as_dict()\n",
    "KALMAN_HEURISTICS_FUNC = KALMAN_FILTER_CONFIG.heuristics.as_dict()\n",
    "\n",
    "CAMERA_PARAMS_PATH = os.path.join(\n",
    "    samples_folder,\n",
    "    'calibration_fake.json',\n",
    ")\n",
    "\n",
    "image_size, intrinsic = utils_camera_systems.get_camera_params(CAMERA_PARAMS_PATH)\n",
    "camera_systems = utils_camera_systems.CameraSystems(image_size, intrinsic)\n",
    "depth_extractor = utils_camera_systems.DepthExtractor(WINDOW_SIZE)\n",
    "\n",
    "kfs = []\n",
    "for i in range(utils_unified_format.TOTAL_POINTS_COUNT):\n",
    "    point = i\n",
    "    if point >= 18:\n",
    "        point = 4\n",
    "    params = KALMAN_FILTER_CONFIG.init_params.as_dict()\n",
    "    params['sigma_u'] = params.pop('sigma_u_points')[point]\n",
    "    params['init_Q'] = np.copy(params['init_Q']) * (params['sigma_u'] ** 2)\n",
    "    kfs.append(utils_kalman_filter.KalmanFilter(**params, **KALMAN_HEURISTICS_FUNC))\n",
    "kalman_filters = utils_kalman_filter.KalmanFilters(kfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (positional_embeddings): PositionalEncoding()\n",
       "  (linear1): Linear(in_features=30, out_features=256, bias=True)\n",
       "  (lstm1): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "  (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep = TRAIN_CONFIG.transforms_params.to_keep\n",
    "\n",
    "test_transforms = transforms.TestStreamTransforms(\n",
    "    to_keep=to_keep,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = classifiers.LSTMClassifier(sum(to_keep), len(label_map))\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_stream_file = os.path.join(\n",
    "    samples_folder,\n",
    "    'color.mkv',\n",
    ")\n",
    "depth_stream_file = os.path.join(\n",
    "    samples_folder,\n",
    "    'depth.mkv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 627 is out of bounds for axis 0 with size 576",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 40\u001b[0m\n\u001b[1;32m     30\u001b[0m frame_points \u001b[39m=\u001b[39m camera_systems\u001b[39m.\u001b[39mzero_points_outside_screen(\n\u001b[1;32m     31\u001b[0m     frame_points,\n\u001b[1;32m     32\u001b[0m     is_normalized\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m frame_points \u001b[39m=\u001b[39m camera_systems\u001b[39m.\u001b[39mnormalized_to_screen(\n\u001b[1;32m     36\u001b[0m     frame_points,\n\u001b[1;32m     37\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m depths \u001b[39m=\u001b[39m depth_extractor\u001b[39m.\u001b[39;49mget_depth_in_window(\n\u001b[1;32m     41\u001b[0m     depth_image,\n\u001b[1;32m     42\u001b[0m     frame_points,\n\u001b[1;32m     43\u001b[0m     predicted,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m predicted \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     kalman_filters\u001b[39m.\u001b[39mreset([\n\u001b[1;32m     48\u001b[0m         np\u001b[39m.\u001b[39marray([[point], [\u001b[39m0\u001b[39m]])\n\u001b[1;32m     49\u001b[0m         \u001b[39mfor\u001b[39;00m point \u001b[39min\u001b[39;00m depths\n\u001b[1;32m     50\u001b[0m     ])\n",
      "File \u001b[0;32m~/pose_classifier/utils/utils_camera_systems.py:177\u001b[0m, in \u001b[0;36mDepthExtractor.get_depth_in_window\u001b[0;34m(self, depth_image, points, predicted)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_depth_in_window\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    172\u001b[0m     depth_image: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m    173\u001b[0m     points: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m    174\u001b[0m     predicted: tp\u001b[39m.\u001b[39mOptional[tp\u001b[39m.\u001b[39mSequence[\u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    175\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    176\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m predicted \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_depth(depth_image, points)\n\u001b[1;32m    179\u001b[0m     low \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    180\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m-\u001b[39m low\n",
      "File \u001b[0;32m~/pose_classifier/utils/utils_camera_systems.py:164\u001b[0m, in \u001b[0;36mDepthExtractor.get_depth\u001b[0;34m(self, depth_image, points)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_depth\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     depth_image: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m    162\u001b[0m     points: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m    163\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m--> 164\u001b[0m     points_depths \u001b[39m=\u001b[39m depth_image[\n\u001b[1;32m    165\u001b[0m         points[:, \u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39;49m),\n\u001b[1;32m    166\u001b[0m         points[:, \u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39;49m),\n\u001b[1;32m    167\u001b[0m     ]\n\u001b[1;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m points_depths\n",
      "\u001b[0;31mIndexError\u001b[0m: index 627 is out of bounds for axis 0 with size 576"
     ]
    }
   ],
   "source": [
    "color_cap = cv2.VideoCapture(color_stream_file)\n",
    "depth_cap = cv2.VideoCapture(depth_stream_file)\n",
    "\n",
    "mp_solver.reset()\n",
    "predicted = None\n",
    "\n",
    "while color_cap.isOpened() and depth_cap.isOpened():\n",
    "    color_ret, color_image = color_cap.read()\n",
    "    depth_ret, depth_image = depth_cap.read()\n",
    "    if color_ret and depth_ret:\n",
    "        depth_image = depth_image.T\n",
    "        ### MediaPipe Extractor\n",
    "        ### ------------------------------\n",
    "        landmarks = mp_solver.process(color_image)\n",
    "        joined_landmarks = itertools.chain(\n",
    "            landmarks.pose_landmarks.landmark if landmarks.pose_landmarks is not None else utils_mediapipe.EMPTY_POSE,\n",
    "            landmarks.left_hand_landmarks.landmark if landmarks.left_hand_landmarks is not None else utils_mediapipe.EMPTY_HAND,\n",
    "            landmarks.right_hand_landmarks.landmark if landmarks.right_hand_landmarks is not None else utils_mediapipe.EMPTY_HAND,\n",
    "        )\n",
    "        frame_points = utils_mediapipe.landmarks_to_array(joined_landmarks)[:, :3]\n",
    "        mp_points = frame_points.reshape(-1)\n",
    "\n",
    "        ### Filtration\n",
    "        ### ------------------------------\n",
    "        mp_points = utils_mediapipe.mediapipe_to_unified(\n",
    "            mp_points.reshape(-1, utils_mediapipe.TOTAL_POINTS_COUNT, 3)\n",
    "        ).reshape(-1, 3 * utils_unified_format.TOTAL_POINTS_COUNT)\n",
    "\n",
    "        frame_points = mp_points.reshape(-1, 3)\n",
    "        frame_points = camera_systems.zero_points_outside_screen(\n",
    "            frame_points,\n",
    "            is_normalized=True,\n",
    "            inplace=True,\n",
    "        )\n",
    "        frame_points = camera_systems.normalized_to_screen(\n",
    "            frame_points,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # depths = depth_extractor.get_depth_in_window(\n",
    "        #     depth_image,\n",
    "        #     frame_points,\n",
    "        #     predicted,\n",
    "        # )\n",
    "\n",
    "        # if predicted is None:\n",
    "        #     kalman_filters.reset([\n",
    "        #         np.array([[point], [0]])\n",
    "        #         for point in depths\n",
    "        #     ])\n",
    "        # depths_filtered = kalman_filters.update(\n",
    "        #     depths,\n",
    "        #     use_heuristic=True,\n",
    "        #     projection=0,\n",
    "        # )\n",
    "\n",
    "        # predicted = kalman_filters.predict(projection=0)\n",
    "        # depths_filtered = tp.cast(tp.List[float], depths_filtered)\n",
    "        # predicted = tp.cast(tp.List[float], predicted)\n",
    "\n",
    "        # frame_points[:, 2] = depths_filtered\n",
    "        # frame_points = camera_systems.screen_to_world(\n",
    "        #     frame_points,\n",
    "        #     inplace=True,\n",
    "        # )\n",
    "\n",
    "        # cv2.imshow('Color Frame', color_frame)\n",
    "        # cv2.imshow('Depth Frame', depth_frame)\n",
    "\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "color_cap.release()\n",
    "depth_cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\n",
    "    samples_folder,\n",
    "    'output.mkv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "playback = pyk4a.PyK4APlayback(filename)\n",
    "\n",
    "playback.open()\n",
    "calib = playback.calibration_raw\n",
    "calib_json = json.dumps(calib)\n",
    "playback.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "playback.open()\n",
    "while True:\n",
    "    try:\n",
    "        frame = playback.get_next_capture()\n",
    "    except EOFError as err:\n",
    "        break\n",
    "playback.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
