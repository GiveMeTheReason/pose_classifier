{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import glob\n",
    "import os\n",
    "import typing as tp\n",
    "import tqdm\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import model.classifiers as classifiers\n",
    "import model.transforms as transforms\n",
    "\n",
    "import utils.utils_camera_systems as utils_camera_systems\n",
    "import utils.utils_kalman_filter as utils_kalman_filter\n",
    "import utils.utils_mediapipe as utils_mediapipe\n",
    "import utils.utils_unified_format as utils_unified_format\n",
    "from config import DATA_CONFIG, TRAIN_CONFIG, KALMAN_FILTER_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (positional_embeddings): PositionalEncoding()\n",
       "  (linear1): Linear(in_features=30, out_features=256, bias=True)\n",
       "  (lstm1): LSTM(256, 256, num_layers=2, batch_first=True)\n",
       "  (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MediaPipe Extractor\n",
    "### ------------------------------\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "DEPTH_FOLDER = DATA_CONFIG.dataset.undistorted\n",
    "GESTURES = TRAIN_CONFIG.gesture_set.gestures\n",
    "CAMERA = 'center'\n",
    "\n",
    "mp_solver_settings = dict(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    ")\n",
    "mp_solver = mp_holistic.Holistic(**mp_solver_settings)\n",
    "\n",
    "path_depth = len(DEPTH_FOLDER.split(os.path.sep))\n",
    "# folder_paths = []\n",
    "folder_paths = sorted(glob.glob(os.path.join(\n",
    "    DEPTH_FOLDER,\n",
    "    'G101',\n",
    "    'select',\n",
    "    'right',\n",
    "    'trial1',\n",
    "    f'cam_{CAMERA}',\n",
    ")))\n",
    "# for gesture in GESTURES:\n",
    "#     folder_paths.extend(sorted(glob.glob(os.path.join(\n",
    "#         DEPTH_FOLDER,\n",
    "#         'G*',\n",
    "#         gesture,\n",
    "#         '*',\n",
    "#         'trial*',\n",
    "#         f'cam_{CAMERA}',\n",
    "#     ))))\n",
    "\n",
    "\n",
    "### Filtration\n",
    "### ------------------------------\n",
    "WINDOW_SIZE = 7\n",
    "\n",
    "KALMAN_PARAMS = KALMAN_FILTER_CONFIG.init_params.as_dict()\n",
    "KALMAN_HEURISTICS_FUNC = KALMAN_FILTER_CONFIG.heuristics.as_dict()\n",
    "\n",
    "CAMERA_PARAMS_PATH = DATA_CONFIG.cameras[f'{CAMERA}_camera_params']\n",
    "\n",
    "image_size, intrinsic = utils_camera_systems.get_camera_params(CAMERA_PARAMS_PATH)\n",
    "camera_systems = utils_camera_systems.CameraSystems(image_size, intrinsic)\n",
    "depth_extractor = utils_camera_systems.DepthExtractor(WINDOW_SIZE)\n",
    "\n",
    "kfs = []\n",
    "for i in range(utils_unified_format.TOTAL_POINTS_COUNT):\n",
    "    point = i\n",
    "    if point >= 18:\n",
    "        point = 4\n",
    "    params = KALMAN_FILTER_CONFIG.init_params.as_dict()\n",
    "    params['sigma_u'] = params.pop('sigma_u_points')[point]\n",
    "    params['init_Q'] = np.copy(params['init_Q']) * (params['sigma_u'] ** 2)\n",
    "    kfs.append(utils_kalman_filter.KalmanFilter(**params, **KALMAN_HEURISTICS_FUNC))\n",
    "kalman_filters = utils_kalman_filter.KalmanFilters(kfs)\n",
    "\n",
    "\n",
    "### Classifier Inference\n",
    "### ------------------------------\n",
    "exp_id = 1\n",
    "device = 'cpu'\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    TRAIN_CONFIG.train_params.output_data,\n",
    "    f'experiment_{str(exp_id).zfill(3)}',\n",
    "    'checkpoint.pth',\n",
    ")\n",
    "\n",
    "label_map = TRAIN_CONFIG.gesture_set.label_map\n",
    "inv_label_map = TRAIN_CONFIG.gesture_set.inv_label_map\n",
    "\n",
    "to_keep = TRAIN_CONFIG.transforms_params.to_keep\n",
    "shape_limit = TRAIN_CONFIG.transforms_params.shape_limit\n",
    "\n",
    "test_transforms = transforms.TestTransforms(\n",
    "    to_keep=to_keep,\n",
    "    shape_limit=shape_limit,\n",
    "    device=device,\n",
    ")\n",
    "label_transforms = transforms.LabelsTransforms(\n",
    "    shape_limit=shape_limit,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model = classifiers.LSTMClassifier(sum(to_keep), len(label_map))\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.64s/it]\n"
     ]
    }
   ],
   "source": [
    "timings_mp = []\n",
    "timings_filter = []\n",
    "timings_model = []\n",
    "\n",
    "for trial_path in tqdm.tqdm(folder_paths):\n",
    "    color_paths = sorted(glob.glob(os.path.join(trial_path, 'color', '*.jpg')))\n",
    "    depth_paths = sorted(glob.glob(os.path.join(trial_path, 'depth', '*.png')))\n",
    "\n",
    "    path_info = color_paths[0].split(os.path.sep)\n",
    "    path_info[path_depth+1] = path_info[path_depth+1].replace('_', '-')\n",
    "\n",
    "    ### MediaPipe Extractor\n",
    "    ### ------------------------------\n",
    "    mp_solver.reset()\n",
    "\n",
    "    ### Filtration\n",
    "    ### ------------------------------\n",
    "    predicted = None\n",
    "\n",
    "\n",
    "    for i, (image_path, depth_path) in enumerate(zip(color_paths, depth_paths)):\n",
    "        color_image = iio.imread(image_path)\n",
    "        depth_image = iio.imread(depth_path).T\n",
    "\n",
    "        ### MediaPipe Extractor\n",
    "        ### ------------------------------\n",
    "        start_ts = time.time()  ###\n",
    "\n",
    "        landmarks = mp_solver.process(color_image)\n",
    "\n",
    "        joined_landmarks = itertools.chain(\n",
    "            landmarks.pose_landmarks.landmark if landmarks.pose_landmarks is not None else utils_mediapipe.EMPTY_POSE,\n",
    "            landmarks.left_hand_landmarks.landmark if landmarks.left_hand_landmarks is not None else utils_mediapipe.EMPTY_HAND,\n",
    "            landmarks.right_hand_landmarks.landmark if landmarks.right_hand_landmarks is not None else utils_mediapipe.EMPTY_HAND,\n",
    "        )\n",
    "        frame_points = utils_mediapipe.landmarks_to_array(joined_landmarks)[:, :3]\n",
    "        mp_points = frame_points.reshape(-1)\n",
    "\n",
    "        end_ts = time.time()  ###\n",
    "        timings_mp.append(end_ts - start_ts)  ###\n",
    "\n",
    "\n",
    "        ### Filtration\n",
    "        ### ------------------------------\n",
    "        start_ts = time.time()  ###\n",
    "\n",
    "        mp_points = utils_mediapipe.mediapipe_to_unified(\n",
    "            mp_points.reshape(-1, utils_mediapipe.TOTAL_POINTS_COUNT, 3)\n",
    "        ).reshape(-1, 3 * utils_unified_format.TOTAL_POINTS_COUNT)\n",
    "\n",
    "        frame_points = mp_points.reshape(-1, 3)\n",
    "        frame_points = camera_systems.zero_points_outside_screen(\n",
    "            frame_points,\n",
    "            is_normalized=True,\n",
    "            inplace=True,\n",
    "        )\n",
    "        frame_points = camera_systems.normalized_to_screen(\n",
    "            frame_points,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        depths = depth_extractor.get_depth_in_window(\n",
    "            depth_image,\n",
    "            frame_points,\n",
    "            predicted,\n",
    "        )\n",
    "\n",
    "        if predicted is None:\n",
    "            kalman_filters.reset([\n",
    "                np.array([[point], [0]])\n",
    "                for point in depths\n",
    "            ])\n",
    "        depths_filtered = kalman_filters.update(\n",
    "            depths,\n",
    "            use_heuristic=True,\n",
    "            projection=0,\n",
    "        )\n",
    "\n",
    "        predicted = kalman_filters.predict(projection=0)\n",
    "        depths_filtered = tp.cast(tp.List[float], depths_filtered)\n",
    "        predicted = tp.cast(tp.List[float], predicted)\n",
    "\n",
    "        frame_points[:, 2] = depths_filtered\n",
    "        frame_points = camera_systems.screen_to_world(\n",
    "            frame_points,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        end_ts = time.time()  ###\n",
    "        timings_filter.append(end_ts - start_ts)  ###\n",
    "\n",
    "\n",
    "        ### Classifier Inference\n",
    "        ### ------------------------------\n",
    "        start_ts = time.time()  ###\n",
    "\n",
    "        points = test_transforms(frame_points.flatten()[None, ...])\n",
    "        with torch.no_grad():\n",
    "            prediction = model(points, use_hidden=True)\n",
    "\n",
    "        prediction_probs, prediction_labels = prediction.max(dim=-1)\n",
    "\n",
    "        end_ts = time.time()  ###\n",
    "        timings_model.append(end_ts - start_ts)  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP:  0.05851172415678166\n",
      "Filter:  0.0029274510943199977\n",
      "Model:  0.0015342905501688807\n"
     ]
    }
   ],
   "source": [
    "if timings_mp:\n",
    "    print('MP: ', sum(timings_mp) / len(timings_mp))\n",
    "if timings_filter:\n",
    "    print('Filter: ', sum(timings_filter) / len(timings_filter))\n",
    "if timings_model:\n",
    "    print('Model: ', sum(timings_model) / len(timings_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
